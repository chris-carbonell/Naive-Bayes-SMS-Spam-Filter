# Naive Bayes SMS Spam Filter

## Goal

* Build a spam filter for an SMS data set using a bag-of-words model
* Compare the efficiency of a unigram model vs a bigram model

## Abstract



## Data

I found the data here: [https://www.kaggle.com/uciml/sms-spam-collection-dataset/home](https://www.kaggle.com/uciml/sms-spam-collection-dataset/home)

## Visualizations

## Conclusion

## Helpful Links
* [https://towardsdatascience.com/spam-or-ham-introduction-to-natural-language-processing-part-2-a0093185aebd](https://towardsdatascience.com/spam-or-ham-introduction-to-natural-language-processing-part-2-a0093185aebd)
* [https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html](https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html)
* [https://machinelearningmastery.com/clean-text-machine-learning-python/](https://machinelearningmastery.com/clean-text-machine-learning-python/)
* [https://scikit-learn.org/stable/modules/feature_extraction.html](https://scikit-learn.org/stable/modules/feature_extraction.html)
* [https://stackoverflow.com/questions/37651057/generate-bigrams-with-nltk](https://stackoverflow.com/questions/37651057/generate-bigrams-with-nltk)
